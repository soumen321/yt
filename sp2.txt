"""
Parallel Alert Processor Lambda Function with ThreadPoolExecutor
Optimized for processing 70 alerts with 10 concurrent workers
"""

import json
import boto3
import requests
import time
import logging
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Thread-local storage for session management
thread_local = threading.local()

def get_alerts_from_dynamodb():
    """Get all enabled alerts from DynamoDB"""
    dynamodb = boto3.resource('dynamodb')
    table_name = os.environ['DYNAMODB_TABLE_NAME']
    table = dynamodb.Table(table_name)
    
    try:
        response = table.scan(
            FilterExpression='enabled = :enabled',
            ExpressionAttributeValues={':enabled': True}
        )
        
        alerts = response['Items']
        logger.info(f"Retrieved {len(alerts)} alerts from DynamoDB")
        return alerts
        
    except Exception as e:
        logger.error(f"Error getting alerts from DynamoDB: {e}")
        raise

def get_thread_session():
    """Get thread-local session for Splunk authentication"""
    if not hasattr(thread_local, 'session_key'):
        thread_local.session_key = authenticate_splunk()
    return thread_local.session_key

def authenticate_splunk():
    """Authenticate with Splunk and get session token"""
    splunk_url = os.environ['SPLUNK_BASE_URL']
    username = os.environ['SPLUNK_USERNAME']
    password = os.environ['SPLUNK_PASSWORD']
    
    auth_url = f"{splunk_url}/services/auth/login"
    auth_data = {
        'username': username,
        'password': password
    }
    
    try:
        response = requests.post(auth_url, data=auth_data, timeout=30)
        response.raise_for_status()
        
        # Parse session key from response
        import re
        session_key_match = re.search(r'<sessionKey>(.*?)</sessionKey>', response.text)
        if session_key_match:
            session_key = session_key_match.group(1)
            logger.info(f"Thread {threading.current_thread().name}: Successfully authenticated with Splunk")
            return session_key
        else:
            raise Exception("Could not extract session key from Splunk response")
            
    except Exception as e:
        logger.error(f"Thread {threading.current_thread().name}: Splunk authentication error: {e}")
        raise

def get_alert_details_from_splunk(alert_name, session_key):
    """Get alert details including the actual query from Splunk"""
    splunk_url = os.environ['SPLUNK_BASE_URL']
    
    # First, try to get the alert from saved searches
    saved_searches_url = f"{splunk_url}/servicesNS/admin/search/saved/searches"
    headers = {
        'Authorization': f'Splunk {session_key}',
        'Content-Type': 'application/x-www-form-urlencoded'
    }
    
    params = {
        'output_mode': 'json',
        'search': f'name="{alert_name}"'
    }
    
    try:
        logger.info(f"Thread {threading.current_thread().name}: Getting alert details for: {alert_name}")
        response = requests.get(saved_searches_url, headers=headers, params=params, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        
        if data.get('entry'):
            alert_entry = data['entry'][0]
            alert_content = alert_entry.get('content', {})
            
            # Extract the search query
            search_query = alert_content.get('search', '')
            if search_query:
                logger.info(f"Thread {threading.current_thread().name}: Found query for alert {alert_name}: {search_query[:100]}...")
                return {
                    'query': search_query,
                    'description': alert_content.get('description', ''),
                    'cron_schedule': alert_content.get('cron_schedule', ''),
                    'alert_type': alert_content.get('alert_type', ''),
                    'alert_threshold': alert_content.get('alert_threshold', '')
                }
            else:
                logger.warning(f"Thread {threading.current_thread().name}: No search query found for alert {alert_name}")
                return None
        else:
            logger.warning(f"Thread {threading.current_thread().name}: No alert found with name: {alert_name}")
            return None
            
    except Exception as e:
        logger.error(f"Thread {threading.current_thread().name}: Error getting alert details for {alert_name}: {e}")
        return None

def execute_splunk_query(query, session_key, timeout=300):
    """Execute Splunk search query and return results"""
    splunk_url = os.environ['SPLUNK_BASE_URL']
    
    # Create search job
    search_url = f"{splunk_url}/services/search/jobs"
    headers = {
        'Authorization': f'Splunk {session_key}',
        'Content-Type': 'application/x-www-form-urlencoded'
    }
    
    search_data = {
        'search': query,
        'output_mode': 'json',
        'exec_mode': 'blocking',
        'timeout': str(timeout)
    }
    
    try:
        logger.info(f"Thread {threading.current_thread().name}: Executing Splunk query: {query[:100]}...")
        start_time = time.time()
        
        # Submit the search job
        response = requests.post(search_url, headers=headers, data=search_data, timeout=timeout+30)
        response.raise_for_status()
        
        job_data = response.json()
        job_id = job_data.get('sid')
        
        if not job_id:
            raise Exception("No job ID returned from Splunk")
        
        logger.info(f"Thread {threading.current_thread().name}: Search job created with ID: {job_id}")
        
        # Wait for job completion and get results
        results = get_search_results(job_id, session_key, timeout)
        execution_time = time.time() - start_time
        
        logger.info(f"Thread {threading.current_thread().name}: Query executed successfully in {execution_time:.2f} seconds")
        
        return {
            'job_id': job_id,
            'results': results,
            'execution_time': execution_time,
            'status': 'completed'
        }
        
    except requests.exceptions.Timeout:
        logger.warning(f"Thread {threading.current_thread().name}: Search query timed out after {timeout} seconds")
        return {
            'status': 'timeout',
            'execution_time': timeout,
            'error': 'Query execution timed out'
        }
    except Exception as e:
        logger.error(f"Thread {threading.current_thread().name}: Error executing search query: {e}")
        raise

def get_search_results(job_id, session_key, timeout=300):
    """Get results from completed search job"""
    splunk_url = os.environ['SPLUNK_BASE_URL']
    results_url = f"{splunk_url}/services/search/jobs/{job_id}/results"
    
    headers = {
        'Authorization': f'Splunk {session_key}',
        'Content-Type': 'application/x-www-form-urlencoded'
    }
    
    params = {
        'output_mode': 'json',
        'count': '1000'  # Adjust based on your needs
    }
    
    # Poll for job completion
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            response = requests.get(results_url, headers=headers, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            # Check if job is done
            if data.get('results'):
                logger.info(f"Thread {threading.current_thread().name}: Job {job_id} completed with {len(data['results'])} results")
                return data
            else:
                # Job might still be running, check status
                status_url = f"{splunk_url}/services/search/jobs/{job_id}"
                status_response = requests.get(status_url, headers=headers, timeout=30)
                status_data = status_response.json()
                
                job_status = status_data.get('entry', [{}])[0].get('content', {}).get('isDone', False)
                if job_status:
                    logger.info(f"Thread {threading.current_thread().name}: Job {job_id} completed")
                    return data
                
                logger.info(f"Thread {threading.current_thread().name}: Job {job_id} still running, waiting...")
                time.sleep(5)  # Wait 5 seconds before checking again
                
        except Exception as e:
            logger.error(f"Thread {threading.current_thread().name}: Error checking job status: {e}")
            time.sleep(5)
    
    raise Exception(f"Search job {job_id} did not complete within {timeout} seconds")

def determine_alert_status(query_results, alert_config):
    """Determine if alert should be OK or KO based on results"""
    if query_results.get('status') == 'timeout':
        return 'TIMEOUT'
    
    if not query_results.get('results'):
        return 'ERROR'
    
    results = query_results['results']
    
    # Check if there are any results (indicating issues)
    if isinstance(results, dict) and results.get('results'):
        result_count = len(results['results'])
        if result_count > 0:
            logger.info(f"Thread {threading.current_thread().name}: Found {result_count} results indicating issues")
            return 'KO'
    
    # If no results found, alert is OK
    logger.info(f"Thread {threading.current_thread().name}: No issues found in query results")
    return 'OK'

def create_servicenow_ticket(alert_name, status, message, execution_time):
    """Create ServiceNow ticket for failed alerts"""
    if status != 'KO':
        return None
    
    servicenow_url = os.environ['SERVICENOW_INSTANCE_URL']
    username = os.environ['SERVICENOW_USERNAME']
    password = os.environ['SERVICENOW_PASSWORD']
    
    url = f"{servicenow_url}/api/now/table/incident"
    headers = {
        'Content-Type': 'application/json',
        'Accept': 'application/json'
    }
    
    auth = (username, password)
    
    incident_data = {
        'short_description': f'Alert Failed: {alert_name}',
        'description': f"""
Alert Name: {alert_name}
Status: {status}
Message: {message}
Execution Time: {execution_time:.2f} seconds
Timestamp: {datetime.utcnow().isoformat()}
Thread: {threading.current_thread().name}
        """.strip(),
        'category': 'Monitoring',
        'subcategory': 'Alert Failure',
        'priority': '3',  # Medium priority
        'urgency': '3',
        'impact': '3',
        'assignment_group': 'IT Operations',
        'state': '1'  # New
    }
    
    try:
        response = requests.post(url, headers=headers, json=incident_data, auth=auth, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        ticket_id = data['result']['sys_id']
        logger.info(f"Thread {threading.current_thread().name}: Created ServiceNow incident {ticket_id} for alert {alert_name}")
        return ticket_id
        
    except Exception as e:
        logger.error(f"Thread {threading.current_thread().name}: Error creating ServiceNow incident: {e}")
        return None

def update_alert_status_in_dynamodb(alert_id, status, last_run):
    """Update alert status in DynamoDB"""
    dynamodb = boto3.resource('dynamodb')
    table_name = os.environ['DYNAMODB_TABLE_NAME']
    table = dynamodb.Table(table_name)
    
    try:
        table.update_item(
            Key={'alert_id': alert_id},
            UpdateExpression='SET last_status = :status, last_run = :last_run',
            ExpressionAttributeValues={
                ':status': status,
                ':last_run': last_run
            }
        )
        logger.info(f"Thread {threading.current_thread().name}: Updated alert {alert_id} status to {status}")
    except Exception as e:
        logger.error(f"Thread {threading.current_thread().name}: Error updating alert status: {e}")

def process_single_alert(alert_config):
    """Process a single alert - optimized for parallel execution"""
    alert_name = alert_config['alert_name']
    alert_id = alert_config['alert_id']
    
    try:
        logger.info(f"Thread {threading.current_thread().name}: Processing alert: {alert_name}")
        
        # Get thread-local session key
        session_key = get_thread_session()
        
        # Get alert details from Splunk
        alert_details = get_alert_details_from_splunk(alert_name, session_key)
        
        if not alert_details:
            logger.error(f"Thread {threading.current_thread().name}: Could not get alert details for {alert_name}")
            return {
                'alert_name': alert_name,
                'status': 'ERROR',
                'message': 'Could not retrieve alert details from Splunk',
                'execution_time': 0,
                'ticket_created': False,
                'thread_id': threading.current_thread().name
            }
        
        # Execute the query
        query_results = execute_splunk_query(alert_details['query'], session_key, timeout=300)
        
        # Determine alert status
        status = determine_alert_status(query_results, alert_config)
        
        # Create message
        if status == 'OK':
            message = "Alert check passed - no issues found"
        elif status == 'KO':
            result_count = 0
            if query_results.get('results') and isinstance(query_results['results'], dict):
                result_count = len(query_results['results'].get('results', []))
            message = f"Alert check failed - {result_count} issues found"
        elif status == 'TIMEOUT':
            message = "Alert check timed out"
        else:
            message = "Alert check encountered an error"
        
        # Create ServiceNow ticket if alert failed
        ticket_id = None
        ticket_created = False
        if status == 'KO':
            ticket_id = create_servicenow_ticket(
                alert_name, status, message, query_results.get('execution_time', 0)
            )
            ticket_created = ticket_id is not None
        
        # Update DynamoDB
        update_alert_status_in_dynamodb(alert_id, status, datetime.utcnow().isoformat())
        
        logger.info(f"Thread {threading.current_thread().name}: Completed processing alert {alert_name}: {status}")
        
        return {
            'alert_name': alert_name,
            'status': status,
            'message': message,
            'execution_time': query_results.get('execution_time', 0),
            'ticket_created': ticket_created,
            'ticket_id': ticket_id,
            'query_used': alert_details['query'],
            'thread_id': threading.current_thread().name
        }
        
    except Exception as e:
        logger.error(f"Thread {threading.current_thread().name}: Error processing alert {alert_name}: {e}")
        return {
            'alert_name': alert_name,
            'status': 'ERROR',
            'message': f"Error processing alert: {str(e)}",
            'execution_time': 0,
            'ticket_created': False,
            'error_details': str(e),
            'thread_id': threading.current_thread().name
        }

def lambda_handler(event, context):
    """Main Lambda handler function with parallel processing"""
    start_time = time.time()
    
    try:
        logger.info("Starting parallel alert processing")
        
        # Get all alerts from DynamoDB
        alerts = get_alerts_from_dynamodb()
        
        if not alerts:
            logger.warning("No alerts found in DynamoDB")
            return {
                'statusCode': 200,
                'body': json.dumps({
                    'message': 'No alerts to process',
                    'processed_count': 0
                })
            }
        
        # Configuration for parallel processing
        MAX_WORKERS = int(os.environ.get('MAX_WORKERS', '10'))
        TIMEOUT_PER_ALERT = int(os.environ.get('TIMEOUT_PER_ALERT', '300'))  # 5 minutes
        
        logger.info(f"Processing {len(alerts)} alerts with {MAX_WORKERS} workers")
        
        # Process alerts in parallel using ThreadPoolExecutor
        results = []
        status_counts = {'OK': 0, 'KO': 0, 'ERROR': 0, 'TIMEOUT': 0}
        ticket_count = 0
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # Submit all tasks
            future_to_alert = {
                executor.submit(process_single_alert, alert_config): alert_config 
                for alert_config in alerts
            }
            
            # Collect results as they complete
            completed_count = 0
            for future in as_completed(future_to_alert, timeout=TIMEOUT_PER_ALERT * len(alerts)):
                try:
                    result = future.result(timeout=TIMEOUT_PER_ALERT)
                    results.append(result)
                    
                    # Count statuses
                    status_counts[result['status']] = status_counts.get(result['status'], 0) + 1
                    if result.get('ticket_created'):
                        ticket_count += 1
                    
                    completed_count += 1
                    logger.info(f"Completed {completed_count}/{len(alerts)} alerts")
                    
                except Exception as e:
                    alert_config = future_to_alert[future]
                    logger.error(f"Error processing alert {alert_config['alert_name']}: {e}")
                    results.append({
                        'alert_name': alert_config['alert_name'],
                        'status': 'ERROR',
                        'message': f"Error: {str(e)}",
                        'execution_time': 0,
                        'ticket_created': False,
                        'thread_id': 'error'
                    })
                    status_counts['ERROR'] += 1
        
        # Calculate summary
        total_execution_time = time.time() - start_time
        
        summary = {
            'total_alerts': len(alerts),
            'processed_alerts': len(results),
            'status_breakdown': status_counts,
            'tickets_created': ticket_count,
            'total_execution_time': total_execution_time,
            'average_time_per_alert': total_execution_time / len(results) if results else 0,
            'max_workers': MAX_WORKERS,
            'parallel_efficiency': f"{(len(results) / total_execution_time):.2f} alerts/second"
        }
        
        logger.info(f"Parallel alert processing completed: {summary}")
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Parallel alert processing completed successfully',
                'summary': summary,
                'results': results
            })
        }
        
    except Exception as e:
        logger.error(f"Fatal error in parallel alert processing: {e}")
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': 'Internal server error',
                'message': str(e)
            })
        }

if __name__ == "__main__":
    # For local testing
    import os
    os.environ.update({
        'DYNAMODB_TABLE_NAME': 'alerts-table',
        'SPLUNK_BASE_URL': 'https://your-splunk-instance.com:8089',
        'SPLUNK_USERNAME': 'your-username',
        'SPLUNK_PASSWORD': 'your-password',
        'SERVICENOW_INSTANCE_URL': 'https://your-instance.service-now.com',
        'SERVICENOW_USERNAME': 'your-username',
        'SERVICENOW_PASSWORD': 'your-password',
        'MAX_WORKERS': '10',
        'TIMEOUT_PER_ALERT': '300'
    })
    
    result = lambda_handler({}, {})
    print(json.dumps(result, indent=2))
